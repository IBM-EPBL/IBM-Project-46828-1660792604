{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwgO-KmlWbkl",
        "outputId": "12ccc368-1623-44b3-a185-4ba44b440298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4317 images belonging to 5 classes.\n",
            "Epoch 1/25\n",
            "25/25 [==============================] - 78s 3s/step - loss: 1.5815 - accuracy: 0.3120\n",
            "Epoch 2/25\n",
            "25/25 [==============================] - 49s 2s/step - loss: 1.2862 - accuracy: 0.4147\n",
            "Epoch 3/25\n",
            "25/25 [==============================] - 47s 2s/step - loss: 1.1897 - accuracy: 0.4933\n",
            "Epoch 4/25\n",
            "25/25 [==============================] - 42s 2s/step - loss: 1.1727 - accuracy: 0.4893\n",
            "Epoch 5/25\n",
            "25/25 [==============================] - 39s 2s/step - loss: 1.1470 - accuracy: 0.5347\n",
            "Epoch 6/25\n",
            "25/25 [==============================] - 39s 2s/step - loss: 1.1480 - accuracy: 0.5293\n",
            "Epoch 7/25\n",
            "25/25 [==============================] - 32s 1s/step - loss: 1.0531 - accuracy: 0.5747\n",
            "Epoch 8/25\n",
            "25/25 [==============================] - 36s 1s/step - loss: 1.0597 - accuracy: 0.5680\n",
            "Epoch 9/25\n",
            "25/25 [==============================] - 36s 1s/step - loss: 1.0567 - accuracy: 0.5944\n",
            "Epoch 10/25\n",
            "25/25 [==============================] - 36s 1s/step - loss: 0.9687 - accuracy: 0.6307\n",
            "Epoch 11/25\n",
            "25/25 [==============================] - 33s 1s/step - loss: 0.9607 - accuracy: 0.6305\n",
            "Epoch 12/25\n",
            "25/25 [==============================] - 35s 1s/step - loss: 0.9507 - accuracy: 0.6227\n",
            "Epoch 13/25\n",
            "25/25 [==============================] - 38s 1s/step - loss: 0.9891 - accuracy: 0.6024\n",
            "Epoch 14/25\n",
            "25/25 [==============================] - 36s 1s/step - loss: 1.0418 - accuracy: 0.5920\n",
            "Epoch 15/25\n",
            "25/25 [==============================] - 42s 2s/step - loss: 0.9380 - accuracy: 0.6440\n",
            "Epoch 16/25\n",
            "25/25 [==============================] - 29s 1s/step - loss: 0.9303 - accuracy: 0.6347\n",
            "Epoch 17/25\n",
            "25/25 [==============================] - 35s 1s/step - loss: 0.9380 - accuracy: 0.6173\n",
            "Epoch 18/25\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.9013 - accuracy: 0.6600\n",
            "Epoch 19/25\n",
            "25/25 [==============================] - 42s 2s/step - loss: 0.8926 - accuracy: 0.6453\n",
            "Epoch 20/25\n",
            "25/25 [==============================] - 40s 2s/step - loss: 0.9308 - accuracy: 0.6360\n",
            "Epoch 21/25\n",
            "25/25 [==============================] - 36s 1s/step - loss: 0.8450 - accuracy: 0.6880\n",
            "Epoch 22/25\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.8694 - accuracy: 0.6680\n",
            "Epoch 23/25\n",
            "25/25 [==============================] - 27s 1s/step - loss: 0.8605 - accuracy: 0.6587\n",
            "Epoch 24/25\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.8651 - accuracy: 0.6760\n",
            "Epoch 25/25\n",
            "25/25 [==============================] - 27s 1s/step - loss: 0.8174 - accuracy: 0.6627\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#from tensorflow.keras.preprocessing.image import image\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,Dense, Flatten, MaxPool2D,Dropout\n",
        "#IMAGE augumentation\n",
        "gen=ImageDataGenerator(\n",
        "    rotation_range=0.2,\n",
        "    rescale=1/255.0,\n",
        "    zoom_range=0.05,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    shear_range=0.05,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "train_generator = gen.flow_from_directory(\n",
        "\"C:\\\\Users\\\\Vetri\\\\Downloads\\\\Flowers-Dataset\\\\flowers\",\n",
        "\ttarget_size=(64,64),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=30\n",
        ")\n",
        "#CREATING A MODEL\n",
        "model = Sequential([\n",
        "    Conv2D(64, (3,3), activation='relu',input_shape=(64,64,3)),\n",
        "    MaxPool2D(2, 2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPool2D(2,2),\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(5, activation='softmax')\n",
        "])\n",
        "#COMPILE THE MODEL\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#FIT THE MODEL TO TRAIN DATASET\n",
        "history = model.fit(train_generator, epochs=25, steps_per_epoch=25, verbose = 1, validation_steps=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moyh9E_1Wbkx"
      },
      "outputs": [],
      "source": [
        "from keras import models #SAVING MODEL AND PREDICTING RESULT WITH THE SAVED MODEL\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import img_to_array, load_img\n",
        "from keras.utils.image_dataset import load_image\n",
        "model.save(\"mod.h5\")\n",
        "md=load_model(\"C:\\\\Users\\\\Vetri\\\\Downloads\\\\Flowers-Dataset\\\\flowers\\\\mod.h5\")\n",
        "img=load_img(\"C:\\\\Users\\\\Vetri\\\\Downloads\\\\Flowers-Dataset\\\\flowers\\\\rose\\\\12240303_80d87f77a3_n.jpg\",target_size=(64,64))\n",
        "x=img_to_array(img)\n",
        "x=np.expand_dims(x,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6MMDDtFWbky",
        "outputId": "c5209689-e75b-435e-f866-809599e8a272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 102ms/step\n"
          ]
        }
      ],
      "source": [
        "r=md.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Krg7TQDWbk0",
        "outputId": "4ec63a23-f1a9-4b09-f669-dd981d1758ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'rose'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels=[\"daisy\",\" dandelion\",\"rose\",\"sunflower\",\"tulip\"]\n",
        "y=np.argmax(r)\n",
        "labels[y]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "865edcce1bdbddb85a9a8c41ab471157080cbac16ba498a594f5ddd004ed46c3"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}